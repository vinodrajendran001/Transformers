{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "NUM_CLS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nr. of training examples 6250\n",
      "- nr. of test examples 6250\n"
     ]
    }
   ],
   "source": [
    "# load the IMDB data\n",
    "final=True\n",
    "vocab_size = 50000\n",
    "batch_size = 4\n",
    "if final:\n",
    "    train, test = datasets.IMDB.splits(text_field=TEXT, label_field=LABEL, root='../data/processed/')\n",
    "\n",
    "    TEXT.build_vocab(train, max_size=vocab_size - 2)\n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size)\n",
    "else:\n",
    "    tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "    TEXT.build_vocab(train, max_size=vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size)\n",
    "\n",
    "print(f'- nr. of training examples {len(train_iter)}')\n",
    "print(f'- nr. of {\"test\" if final else \"validation\"} examples {len(test_iter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_iter:\n",
    "    text, label = i.text, i.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- maximum sequence length: 4940\n"
     ]
    }
   ],
   "source": [
    "max_length = -1\n",
    "if max_length < 0:\n",
    "    mx = max([input.text[0].size(1) for input in train_iter])\n",
    "    mx = mx * 2\n",
    "    print(f'- maximum sequence length: {mx}')\n",
    "else:\n",
    "    mx = max_length\n",
    "    print(f'- maximum sequence length: {mx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-transformer] *",
   "language": "python",
   "name": "conda-env-conda-transformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
