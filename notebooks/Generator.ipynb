{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_XArO51kRSK",
        "outputId": "fde97814-1ed0-49c4-cb96-d3ebf7641ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSSVqbxIkXUD",
        "outputId": "72d843aa-8bcf-4887-c2a5-d968111f6365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AujIe5ZnkU_u"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as dist\n",
        "import random, tqdm, sys, math, gzip, os\n",
        "import numpy as np\n",
        "from utils.util import d, save_checkpoint, load_checkpoint\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GTransformer\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq2-7CTNlD_6"
      },
      "source": [
        "def sample(lnprobs, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample an element from a categorical distribution\n",
        "    :param lnprobs: Outcome log-probabilities\n",
        "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
        "        0.0 returns the maximum probability element.\n",
        "    :return: The index of the sampled element.\n",
        "    \"\"\"\n",
        "\n",
        "    if temperature == 0.0:\n",
        "        return lnprobs.argmax()\n",
        "\n",
        "    p = F.softmax(lnprobs / temperature, dim=0)\n",
        "    cd = dist.Categorical(p)\n",
        "\n",
        "    return cd.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GLx7BjTlJVD"
      },
      "source": [
        "def trainF(model, data_train, data_test, num_batches, refcontext, test_every, test_subset, test_batchsize, criterion, save_name):\n",
        "\n",
        "    gradient_clipping = 1.0\n",
        "    best_bits_per_byte = float(\"Inf\")\n",
        "    total_bits = []\n",
        "\n",
        "\n",
        "    # training loop\n",
        "    # - note: we don't loop over the data, instead we sample a batch of random subsequences each time.\n",
        "    for i in tqdm.trange(num_batches):\n",
        "        # sample a batch of random subsequences\n",
        "        starts = torch.randint(size=(batch_size, ), low=0, high=data_train.size(0) - refcontext - 1)\n",
        "        seqs_source = [data_train[start  :start+refcontext] for start in starts]\n",
        "        seqs_target = [data_train[start+1:start+refcontext+1] for start in starts]\n",
        "        source = torch.cat([s[None, :] for s in seqs_source ], dim=0).to(torch.long)\n",
        "        target = torch.cat([s[None, :] for s in seqs_target ], dim=0).to(torch.long)\n",
        "        # - target is the same sequence as source, except one character ahead\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            source, target = source.cuda(), target.cuda()\n",
        "        source, target = Variable(source), Variable(target)\n",
        "\n",
        "        output = model(source)\n",
        "\n",
        "        loss = criterion(output.transpose(2, 1), target)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()       \n",
        "\n",
        "        # clip gradients\n",
        "        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
        "        if gradient_clipping > 0.0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # - validate every {test_every} steps. First we compute the\n",
        "        #   compression on the validation (or a subset)\n",
        "        #   then we generate some random text to monitor progress\n",
        "        if i != 0 and (i % test_every == 0 or i == num_batches - 1):\n",
        "\n",
        "            upto = data_test.size(0) if i == num_batches - 1 else test_subset\n",
        "            data_sub = data_test[:upto]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                bits, tot = 0.0, 0\n",
        "                batch = [] # buffer, every time it fills up, we run it through the model\n",
        "\n",
        "                for current in range(data_sub.size(0)):\n",
        "\n",
        "                    fr = max(0, current - refcontext)\n",
        "                    to = current + 1\n",
        "\n",
        "                    context = data_sub[fr:to].to(torch.long)\n",
        "                    \n",
        "                    if context.size(0) < refcontext + 1:\n",
        "                        pad = torch.zeros(size=(refcontext + 1 - context.size(0),), dtype=torch.long)\n",
        "                        context = torch.cat([pad, context], dim=0)\n",
        "\n",
        "                        assert context.size(0) == refcontext + 1\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                        context = context.cuda()\n",
        "\n",
        "                    batch.append(context[None, :])\n",
        "\n",
        "                    if len(batch) == test_batchsize or current == data_sub.size(0) - 1:\n",
        "\n",
        "                        # batch is full, run it through the model\n",
        "                        b = len(batch)\n",
        "\n",
        "                        all = torch.cat(batch, dim=0)\n",
        "                        source = all[:, :-1] # input\n",
        "                        target = all[:, -1]  # target values\n",
        "\n",
        "                        output = model(source)\n",
        "\n",
        "                        lnprobs = output[torch.arange(b, device=d()), -1, target]\n",
        "                        log2probs = lnprobs * LOG2E # convert from nats to bits\n",
        "\n",
        "                        bits += - log2probs.sum()\n",
        "                        batch = [] # empty buffer\n",
        "                        \n",
        "\n",
        "                bits_per_byte = bits / data_sub.size(0)\n",
        "\n",
        "                total_bits.append(bits_per_byte)\n",
        "\n",
        "                # print validation performance. 1 bit per byte is (currently) state of the art.\n",
        "                print(f'epoch{i}: {bits_per_byte:.4} bits per byte')\n",
        "\n",
        "                if bits_per_byte < best_bits_per_byte:\n",
        "                  best_bits_per_byte = bits_per_byte\n",
        "                  save_checkpoint(save_name, model, optimizer, best_bits_per_byte)\n",
        "\n",
        "\n",
        "                # generate some random text\n",
        "                GENSIZE = 600\n",
        "                TEMP = 0.5\n",
        "                seedfr = random.randint(0, data_test.size(0) - refcontext)\n",
        "                input = data_test[seedfr:seedfr + refcontext].to(torch.long)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    input = input.cuda()\n",
        "\n",
        "                input = Variable(input)\n",
        "\n",
        "                print('[', end='', flush=True)\n",
        "                for c in input:\n",
        "                    print(str(chr(c)), end='', flush=True)\n",
        "                print(']', end='', flush=True)\n",
        "\n",
        "                for _ in range(GENSIZE):\n",
        "                    output = model(input[None, :])\n",
        "                    c = sample(output[0, -1, :], TEMP)\n",
        "                    print(str(chr(max(32, c))), end='', flush=True)\n",
        "\n",
        "                    input = torch.cat([input[1:], c[None]], dim=0)\n",
        "\n",
        "                print()\n",
        "\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "    return total_bits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZdY9KUylPPc"
      },
      "source": [
        "def eval(model, test_loader):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bits = 0.0\n",
        "        refcontext = 256\n",
        "        # batch is full, run it through the model\n",
        "        for i in tqdm.trange(test_loader.size(0)):\n",
        "          seedfr = random.randint(0, test_loader.size(0) - refcontext)\n",
        "          input = test_loader[seedfr:seedfr + refcontext].to(torch.long)\n",
        "\n",
        "          context = input\n",
        "          b = len(context[None, :])\n",
        "\n",
        "          if context.size(0) < refcontext + 1:\n",
        "              pad = torch.zeros(size=(refcontext + 1 - context.size(0),), dtype=torch.long)\n",
        "              context = torch.cat([pad, context], dim=0)\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "            context = context.cuda()\n",
        "\n",
        "          source = context[None, :-1] # input\n",
        "          target = context[-1]  # target values\n",
        "\n",
        "          output = model(source)\n",
        "\n",
        "          lnprobs = output[torch.arange(b, device=d()), -1, target]\n",
        "          log2probs = lnprobs * LOG2E # convert from nats to bits\n",
        "\n",
        "          bits += - log2probs.sum()\n",
        "\n",
        "        bits_per_byte = bits / len(test_loader)\n",
        "\n",
        "        # print test performance. 1 bit per byte is (currently) state of the art.\n",
        "        print('bits per byte: ', bits_per_byte)\n",
        "\n",
        "        # generate some random text\n",
        "        GENSIZE = 600\n",
        "        TEMP = 0.5\n",
        "        seedfr = random.randint(0, test_loader.size(0) - refcontext)\n",
        "        input = test_loader[seedfr:seedfr + refcontext].to(torch.long)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "\n",
        "        input = Variable(input)\n",
        "\n",
        "        print('[', end='', flush=True)\n",
        "        for c in input:\n",
        "            print(str(chr(c)), end='', flush=True)\n",
        "        print(']', end='', flush=True)\n",
        "\n",
        "        for _ in range(GENSIZE):\n",
        "            output = model(input[None, :])\n",
        "            c = sample(output[0, -1, :], TEMP)\n",
        "            print(str(chr(max(32, c))), end='', flush=True)\n",
        "\n",
        "            input = torch.cat([input[1:], c[None]], dim=0)\n",
        "\n",
        "        print()   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxfqPmaJlmS6"
      },
      "source": [
        "def enwik9(path, n_train=int(90e6), n_valid=int(5e6), n_test=int(5e6)):\n",
        "    \"\"\"\n",
        "    Load the enwik8 dataset from the Hutter challenge.\n",
        "    Adapted from https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py\n",
        "    :param path:\n",
        "    :param n_train:\n",
        "    :param n_valid:\n",
        "    :param n_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(path) as file:\n",
        "        X = np.fromstring(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n",
        "        trX, vaX, teX = np.split(X, [n_train, n_train + n_valid])\n",
        "        return torch.from_numpy(trX), torch.from_numpy(vaX), torch.from_numpy(teX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BSksomtlT6c",
        "outputId": "1f11ddca-e0cb-486b-9b81-a566b56785c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# NB, the enwik8 data contains tokens from 9 to 240, but well round up to the nearest\n",
        "# power of two.\n",
        "NUM_TOKENS = 256\n",
        "# Used for converting between nats and bits\n",
        "LOG2E = math.log2(math.e)\n",
        "\n",
        "data = 'enwik9'\n",
        "\n",
        "data_train, data_val, data_test = enwik9(data)\n",
        "\n",
        "print ('nr of train examples', len(data_train))\n",
        "print ('nr of valid examples', len(data_val))\n",
        "print ('nr of test examples', len(data_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nr of train examples 90000000\n",
            "nr of valid examples 5000000\n",
            "nr of test examples 5379691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsSbgvZQp9H6",
        "outputId": "d62032b7-a345-4475-968a-25336b36df63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_size = 128\n",
        "num_heads = 8\n",
        "depth = 12\n",
        "refcontext = 256\n",
        "wide = True\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# create the model\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = GTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=refcontext, num_tokens=NUM_TOKENS, wide=wide)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model architecture:\\n\\n', model)\n",
        "print(f'\\nThe model has {temp:,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model architecture:\n",
            "\n",
            " GTransformer(\n",
            "  (token_embedding): Embedding(256, 128)\n",
            "  (pos_embedding): Embedding(256, 128)\n",
            "  (tblocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (attention): SelfAttentionWide(\n",
            "        (tokeys): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (toqueries): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (tovalues): Linear(in_features=128, out_features=1024, bias=False)\n",
            "        (unifyheads): Linear(in_features=1024, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (do): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (toprobs): Linear(in_features=128, out_features=256, bias=True)\n",
            ")\n",
            "\n",
            "The model has 7,978,240 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjOgcFRwqXF6"
      },
      "source": [
        "# start training\n",
        "optimizer = torch.optim.Adam(lr = 0.0001, params=model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: min(i / (5000/batch_size), 1.0))\n",
        "criterion = nn.NLLLoss(reduction='mean')\n",
        "num_batches = 10000\n",
        "test_every = 100\n",
        "test_subset = 10000\n",
        "test_batchsize = 64\n",
        "save_path = 'GTransformersNet.pt'\n",
        "bits_per_byte = trainF(model, data_train, data_val, num_batches, refcontext, test_every, test_subset, test_batchsize, criterion, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5T1Gnqm7uFw",
        "outputId": "a61b7b9f-df4b-41c0-feb3-7a486033d49f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "load_model = GTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=refcontext, num_tokens=NUM_TOKENS, wide=wide).to(device)\n",
        "load_optimizer = torch.optim.Adam(load_model.parameters(), lr=0.0001)\n",
        "\n",
        "best_val_loss = load_checkpoint(load_model, save_path, load_optimizer)\n",
        "print(best_val_loss)\n",
        "\n",
        "test_subset = 10000\n",
        "\n",
        "eval(load_model, data_test[:test_subset])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 7/10000 [00:00<02:25, 68.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== GTransformersNet.pt\n",
            "tensor(1.9722, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [01:57<00:00, 85.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bits per byte:  tensor(1.9145, device='cuda:0')\n",
            "[enerator-builders. The ''alternator'' syntax allows a series of items to be generated in sequence"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " until one fails: &lt;code&gt;1 | &quot;hello&quot; | x &lt; 5&lt;/code&gt; can generate &quot;1&quot;, &quot;hello&quot;, and &quot;5&quot; if x is less than ]input shape---> torch.Size([256]) torch.Size([1, 256])\n",
            "the &quot;code of the [[can faility]]&quot;, &quot;code&quot; with the [[control]], [[crossone]]&quot; is the tribe of a change also defined to be the [[epitoday of the illustric to not the controlled in the [[Anthropology]] of [[the Canada]] and [[2005]]. The [[anthropology]] set as a [[Terriba]] and [[Anthropology]] by the [[Anthropology]] part of the [[Stari Countile County of the [[Anthropology]] and [[Anthropology]] in [[Anthropology]].  The set of [[Anthropology]] and [[Anthropology]] was an introduced to the [[Anthropology]] of [[Bary University]], [[Facile Starial]], [[Anthropology]], \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5hM9BGRw8CX",
        "outputId": "06627656-76af-4bd5-d38a-a0636f8b3b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "#plotting of training and validation loss\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('bits per byte')\n",
        "plt.plot(bits_per_byte, label='bits_per_byte')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6b2d46e147a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bits per byte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits_per_byte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bits_per_byte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plt.savefig('reports/figures/lossclassifier.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bits_per_byte' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDpwgJPOHWjN"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG0vKmkk2kWm"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "\n",
        "    context = [ord(c) for c in test_loader]\n",
        "    context = np.asarray(context)\n",
        "    context = torch.from_numpy(context)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        refcontext = 256\n",
        "        # generate some random text\n",
        "        GENSIZE = 600\n",
        "        TEMP = 0.5\n",
        "\n",
        "        if context.size(0) < refcontext + 1:\n",
        "            pad = torch.zeros(size=(refcontext - context.size(0),), dtype=torch.long)\n",
        "            input = torch.cat([pad, context], dim=0)\n",
        "        elif context.size(0) > 256:\n",
        "            input = context[-256:]\n",
        "        else:\n",
        "            input = context\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "\n",
        "        input = Variable(input)\n",
        "\n",
        "        print (f\"Input text considered according to model config....\\n\")\n",
        "\n",
        "        print('[', end='', flush=True)\n",
        "        for c in input:\n",
        "            print(str(chr(c)), end='', flush=True)\n",
        "        print(']', end='', flush=True)\n",
        "\n",
        "        print (f\"\\nGenerated text....\\n\")\n",
        "\n",
        "        for _ in range(GENSIZE):\n",
        "            output = model(input[None, :]).to(device)\n",
        "            c = sample(output[0, -1, :], TEMP)\n",
        "            print(str(chr(max(32, c))), end='', flush=True)\n",
        "\n",
        "            input = torch.cat([input[1:], c[None]], dim=0)\n",
        "\n",
        "        print()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KdhApz22k1K",
        "outputId": "70a2ce49-03ba-4c7d-d315-7564399b9747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "text = \"1228X Human & Rousseau. Because many of his stories were originally published in long-forgotten magazines and journals, there are a number of [[anthology|anthologies]] by different collators each containing a different selection. His original books ha\"\n",
        "# NB, the enwik8 data contains tokens from 9 to 240, but well round up to the nearest\n",
        "# power of two.\n",
        "NUM_TOKENS = 256\n",
        "# Used for converting between nats and bits\n",
        "LOG2E = math.log2(math.e)\n",
        "\n",
        "embedding_size = 128\n",
        "num_heads = 8\n",
        "depth = 12\n",
        "refcontext = 256\n",
        "wide = True\n",
        "\n",
        "#creating the original network\n",
        "save_path = 'GTransformersNet.pt' \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
        "load_model = GTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=refcontext, num_tokens=NUM_TOKENS, wide=wide).to(device)\n",
        "state_dict = torch.load(save_path)\n",
        "load_model.load_state_dict(state_dict['model_state_dict'])\n",
        "load_optimizer = torch.optim.Adam(load_model.parameters(), lr=0.0001)\n",
        "load_optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "val_loss = state_dict['val_loss']\n",
        "print(f'Bits per byte info of loaded model: ',val_loss)\n",
        "\n",
        "predict(load_model, text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bits per byte info of loaded model:  tensor(1.9722, device='cuda:0')\n",
            "Input text considered according to model config....\n",
            "\n",
            "[\u0000\u0000\u0000\u0000\u00001228X Human & Rousseau. Because many of his stories were originally published in long-forgotten magazines and journals, there are a number of [[anthology|anthologies]] by different collators each containing a different selection. His original books ha]\n",
            "Generated text....\n",
            "\n",
            "ve been the selection of the contain of the published and his the formal in [[196]] in [[1983]]. The are contained a most of the [[Articital of Ships|Articital (colland)|Articital of Articital Articital Manage]] between the [[Charles of Articital Charles and Sharet]] in the [[Ships of Articital Charles]]. In [[1822]] the [[Charles of Goth Articital Controlland]] in the [[Articital Colland]] and [[Charles Articital Charles of [[Charles Charles]] and [[Charles International Charles and [[Charles Central Charles]]. In the [[International Community]] and [[Charles Charles]] is the [[Charles Collan\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}